{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "train.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwKlVVHh19aK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5b8a7d80-d685-4993-e9bf-d7aba5972f60"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhu_7Apj3OtE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0c230479-7ae9-472c-d4c9-89466f4791cf"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/Math_to_latex')\n",
        "print(sys.path)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['', '/env/python', '/usr/lib/python36.zip', '/usr/lib/python3.6', '/usr/lib/python3.6/lib-dynload', '/usr/local/lib/python3.6/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.6/dist-packages/IPython/extensions', '/root/.ipython', '/content/drive/My Drive/Math_to_latex']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDi7vhcu1z1g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Python 3.6 \n",
        "Pytorch >= 0.4\n",
        "Written by Hongyu Wang in Beihang university\n",
        "'''\n",
        "import torch\n",
        "import math\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import numpy\n",
        "import torch.utils.data as data\n",
        "from data_iterator import dataIterator\n",
        "from Densenet_torchvision import densenet121\n",
        "from Attention_RNN import AttnDecoderRNN\n",
        "#from Resnet101 import resnet101\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "# if __name__ == '__main__':\n",
        "#     freeze_support()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZlFifO13icM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9e9cdcbe-7fec-498d-ca6c-454e4ee82c92"
      },
      "source": [
        "gpu = torch.cuda.current_device()\n",
        "print(gpu)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsClwMF51z1q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compute the wer loss\n",
        "def cmp_result(label,rec):\n",
        "    dist_mat = numpy.zeros((len(label)+1, len(rec)+1),dtype='int32')\n",
        "    dist_mat[0,:] = range(len(rec) + 1)\n",
        "    dist_mat[:,0] = range(len(label) + 1)\n",
        "    for i in range(1, len(label) + 1):\n",
        "        for j in range(1, len(rec) + 1):\n",
        "            hit_score = dist_mat[i-1, j-1] + (label[i-1] != rec[j-1])\n",
        "            ins_score = dist_mat[i,j-1] + 1\n",
        "            del_score = dist_mat[i-1, j] + 1\n",
        "            dist_mat[i,j] = min(hit_score, ins_score, del_score)\n",
        "    dist = dist_mat[len(label), len(rec)]\n",
        "    return dist, len(label)\n",
        "\n",
        "def load_dict(dictFile):\n",
        "    fp=open(dictFile)\n",
        "    stuff=fp.readlines()\n",
        "    fp.close()\n",
        "    lexicon={}\n",
        "    for l in stuff:\n",
        "        w=l.strip().split()\n",
        "        lexicon[w[0]]=int(w[1])\n",
        "    print('total words/phones',len(lexicon))\n",
        "    return lexicon"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IezuzFh91z19",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "867a3f53-745c-41b9-91ea-39f3a79266c8"
      },
      "source": [
        "PATH = '/content/drive/My Drive/Math_to_latex/'\n",
        "datasets=[f'{PATH}offline-train.pkl',f'{PATH}/train_caption.txt']\n",
        "valid_datasets=[f'{PATH}offline-test.pkl', f'{PATH}test_caption.txt']\n",
        "dictionaries=[f'{PATH}dictionary.txt']\n",
        "batch_Imagesize=500000\n",
        "valid_batch_Imagesize=500000\n",
        "# batch_size for training and testing\n",
        "batch_size=6\n",
        "batch_size_t=6\n",
        "# the max (label length/Image size) in training and testing\n",
        "# you can change 'maxlen','maxImagesize' by the size of your GPU\n",
        "maxlen=48\n",
        "maxImagesize= 100000\n",
        "# hidden_size in RNN\n",
        "hidden_size = 256\n",
        "# teacher_forcing_ratio \n",
        "teacher_forcing_ratio = 1\n",
        "# change the gpu id \n",
        "gpu = [0,1]\n",
        "# learning rate\n",
        "# lr_rate = 0.0001\n",
        "lr_rate = 0.001\n",
        "# flag to remember when to change the learning rate\n",
        "flag = 0\n",
        "# exprate\n",
        "exprate = 0\n",
        "\n",
        "# worddicts\n",
        "worddicts = load_dict(dictionaries[0])\n",
        "worddicts_r = [None] * len(worddicts)\n",
        "for kk, vv in worddicts.items():\n",
        "        worddicts_r[vv] = kk\n",
        "\n",
        "#load train data and test data\n",
        "train,train_label = dataIterator(\n",
        "                                    datasets[0], datasets[1],worddicts,batch_size=1,\n",
        "                                    batch_Imagesize=batch_Imagesize,maxlen=maxlen,maxImagesize=maxImagesize\n",
        "                                 )\n",
        "len_train = len(train)\n",
        "\n",
        "test,test_label = dataIterator(\n",
        "                                    valid_datasets[0],valid_datasets[1],worddicts,batch_size=1,\n",
        "                                    batch_Imagesize=batch_Imagesize,maxlen=maxlen,maxImagesize=maxImagesize\n",
        "                                )\n",
        "len_test = len(test)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total words/phones 112\n",
            "total  8359 batch data loaded\n",
            "ignore 476 images\n",
            "total  925 batch data loaded\n",
            "ignore 61 images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D97Gj3QK1z2F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class custom_dset(data.Dataset):\n",
        "    def __init__(self,train,train_label,batch_size):\n",
        "        self.train = train\n",
        "        self.train_label = train_label\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        train_setting = torch.from_numpy(numpy.array(self.train[index]))\n",
        "        label_setting = torch.from_numpy(numpy.array(self.train_label[index])).type(torch.LongTensor)\n",
        "\n",
        "        size = train_setting.size()\n",
        "        train_setting = train_setting.view(1,size[2],size[3])\n",
        "        label_setting = label_setting.view(-1)\n",
        "        return train_setting,label_setting\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.train)\n",
        "\n",
        "\n",
        "off_image_train = custom_dset(train,train_label,batch_size)\n",
        "off_image_test = custom_dset(test,test_label,batch_size)\n",
        "\n",
        "# collate_fn is writting for padding imgs in batch. \n",
        "# As images in my dataset are different size, so the padding is necessary.\n",
        "# Padding images to the max image size in a mini-batch and cat a mask. \n",
        "def collate_fn(batch):\n",
        "    batch.sort(key=lambda x: len(x[1]), reverse=True)\n",
        "    img, label = zip(*batch)\n",
        "    aa1 = 0\n",
        "    bb1 = 0\n",
        "    k = 0\n",
        "    k1 = 0\n",
        "    max_len = len(label[0])+1\n",
        "    for j in range(len(img)):\n",
        "        size = img[j].size()\n",
        "        if size[1] > aa1:\n",
        "            aa1 = size[1]\n",
        "        if size[2] > bb1:\n",
        "            bb1 = size[2]\n",
        "\n",
        "    for ii in img:\n",
        "        ii = ii.float()\n",
        "        img_size_h = ii.size()[1]\n",
        "        img_size_w = ii.size()[2]\n",
        "        img_mask_sub_s = torch.ones(1,img_size_h,img_size_w).type(torch.FloatTensor)\n",
        "        img_mask_sub_s = img_mask_sub_s*255.0\n",
        "        img_mask_sub = torch.cat((ii,img_mask_sub_s),dim=0)\n",
        "        padding_h = aa1-img_size_h\n",
        "        padding_w = bb1-img_size_w\n",
        "        m = torch.nn.ZeroPad2d((0,padding_w,0,padding_h))\n",
        "        img_mask_sub_padding = m(img_mask_sub)\n",
        "        img_mask_sub_padding = img_mask_sub_padding.unsqueeze(0)\n",
        "        if k==0:\n",
        "            img_padding_mask = img_mask_sub_padding\n",
        "        else:\n",
        "            img_padding_mask = torch.cat((img_padding_mask,img_mask_sub_padding),dim=0)\n",
        "        k = k+1\n",
        "\n",
        "    for ii1 in label:\n",
        "        ii1 = ii1.long()\n",
        "        ii1 = ii1.unsqueeze(0)\n",
        "        ii1_len = ii1.size()[1]\n",
        "        m = torch.nn.ZeroPad2d((0,max_len-ii1_len,0,0))\n",
        "        ii1_padding = m(ii1)\n",
        "        if k1 == 0:\n",
        "            label_padding = ii1_padding\n",
        "        else:\n",
        "            label_padding = torch.cat((label_padding,ii1_padding),dim=0)\n",
        "        k1 = k1+1\n",
        "\n",
        "    img_padding_mask = img_padding_mask/255.0\n",
        "    return img_padding_mask, label_padding\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset = off_image_train,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = True,\n",
        "    collate_fn = collate_fn,\n",
        "    # num_workers=2,\n",
        "    )\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataset = off_image_test,\n",
        "    batch_size = batch_size_t,\n",
        "    shuffle = True,\n",
        "    collate_fn = collate_fn,\n",
        "    # num_workers=2,\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41v3UxF71z2K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def my_train(target_length,attn_decoder1,\n",
        "             output_highfeature, output_area,y,criterion,encoder_optimizer1,decoder_optimizer1,x_mean,dense_input,h_mask,w_mask,gpu,\n",
        "             decoder_input,decoder_hidden,attention_sum,decoder_attention):\n",
        "    loss = 0\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    flag_z = [0]*batch_size\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        encoder_optimizer1.zero_grad()\n",
        "        decoder_optimizer1.zero_grad()\n",
        "        my_num = 0\n",
        "\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention, attention_sum = attn_decoder1(decoder_input,\n",
        "                                                                                             decoder_hidden,\n",
        "                                                                                             output_highfeature,\n",
        "                                                                                             output_area,\n",
        "                                                                                             attention_sum,\n",
        "                                                                                             decoder_attention,\n",
        "                                                                                             dense_input,batch_size,h_mask,w_mask,gpu)\n",
        "            \n",
        "            \n",
        "            #print(decoder_output.size()) (batch,1,112)\n",
        "            y = y.unsqueeze(0)\n",
        "            for i in range(batch_size):\n",
        "                if int(y[0][i][di]) == 0:\n",
        "                    flag_z[i] = flag_z[i]+1\n",
        "                    if flag_z[i] > 1:\n",
        "                        continue\n",
        "                    else:\n",
        "                        loss += criterion(decoder_output[i], y[:,i,di])\n",
        "                else:\n",
        "                    loss += criterion(decoder_output[i], y[:,i,di])\n",
        "\n",
        "            if int(y[0][0][di]) == 0:\n",
        "                break\n",
        "            decoder_input = y[:,:,di]\n",
        "            decoder_input = decoder_input.squeeze(0)\n",
        "            y = y.squeeze(0)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        encoder_optimizer1.step()\n",
        "        decoder_optimizer1.step()\n",
        "        return loss.item()\n",
        "\n",
        "    else:\n",
        "        encoder_optimizer1.zero_grad()\n",
        "        decoder_optimizer1.zero_grad()\n",
        "        my_num = 0\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention,attention_sum= attn_decoder1(decoder_input, decoder_hidden,\n",
        "                                                                                output_highfeature, output_area,\n",
        "                                                                                attention_sum,decoder_attention,dense_input,batch_size,\n",
        "                                                                                h_mask,w_mask,gpu)\n",
        "            #print(decoder_output.size()) 1*10*112\n",
        "            #print(y.size())  1*37\n",
        "            #topi (b,1)\n",
        "            topv,topi = torch.max(decoder_output,2)\n",
        "            decoder_input = topi\n",
        "            decoder_input = decoder_input.view(batch_size)\n",
        "\n",
        "            y = y.unsqueeze(0)\n",
        "            #print(y_t)\n",
        "\n",
        "            # 1*bs*17\n",
        "            for k in range(batch_size):\n",
        "                if int(y[0][k][di]) == 0:\n",
        "                    flag_z[k] = flag_z[k]+1\n",
        "                    if flag_z[k] > 1:\n",
        "                        continue\n",
        "                    else:\n",
        "                        loss += criterion(decoder_output[k], y[:,k,di])\n",
        "                else:\n",
        "                    loss += criterion(decoder_output[k], y[:,k,di])\n",
        "\n",
        "            y = y.squeeze(0)\n",
        "            # if int(topi[0]) == 0:\n",
        "            #     break\n",
        "        loss.backward()\n",
        "        encoder_optimizer1.step()\n",
        "        decoder_optimizer1.step()\n",
        "        return loss.item()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2k8uTbz313W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpu = [torch.cuda.current_device()]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVuKzMZe1z2V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = densenet121()\n",
        "\n",
        "pthfile = f'{PATH}densenet121-a639ec97.pth'\n",
        "pretrained_dict = torch.load(pthfile) \n",
        "encoder_dict = encoder.state_dict()\n",
        "pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in encoder_dict}\n",
        "encoder_dict.update(pretrained_dict)\n",
        "encoder.load_state_dict(encoder_dict)\n",
        "\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size,112,dropout_p=0.5)\n",
        "\n",
        "encoder=encoder.cuda() #gpu\n",
        "attn_decoder1 = attn_decoder1.cuda()\n",
        "encoder = torch.nn.DataParallel(encoder, device_ids=gpu)\n",
        "attn_decoder1 = torch.nn.DataParallel(attn_decoder1, device_ids=gpu)\n",
        "\n",
        "# encoder = torch.nn.DataParallel(encoder)\n",
        "# attn_decoder1 = torch.nn.DataParallel(attn_decoder1)\n",
        "attn_decoder1.load_state_dict(torch.load(f'{PATH}model/attn_decoder_v1.pkl'))\n",
        "encoder.load_state_dict(torch.load(f'{PATH}model/encoder_v1.pkl'))\n",
        "\n",
        "def imresize(im,sz):\n",
        "    pil_im = Image.fromarray(im)\n",
        "    return numpy.array(pil_im.resize(sz))\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEVf1EdMmIJC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5a19da82-60e4-442d-98f9-c4ce049f7e3e"
      },
      "source": [
        "lr_rate"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.001"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQ4hWX0x1z2c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "da928c70-65b7-42a4-8a53-4ecdb85e33e2"
      },
      "source": [
        "criterion = nn.NLLLoss()\n",
        "\n",
        "decoder_input_init = torch.LongTensor([111]*batch_size).cuda() #gpu\n",
        "decoder_hidden_init = torch.randn(batch_size, 1, hidden_size).cuda()\n",
        "# decoder_input_init = torch.LongTensor([111]*batch_size)\n",
        "# decoder_hidden_init = torch.randn(batch_size, 1, hidden_size)\n",
        "nn.init.xavier_uniform_(decoder_hidden_init)\n",
        "\n",
        "encoder_optimizer1 = torch.optim.Adam(encoder.parameters(), lr=lr_rate)\n",
        "decoder_optimizer1 = torch.optim.Adam(attn_decoder1.parameters(), lr=lr_rate)\n",
        "\n",
        "for epoch in tqdm(range(5)):\n",
        "#     encoder_optimizer1 = torch.optim.SGD(encoder.parameters(), lr=lr_rate,momentum=0.9)\n",
        "#     decoder_optimizer1 = torch.optim.SGD(attn_decoder1.parameters(), lr=lr_rate,momentum=0.9)\n",
        "\n",
        "    # # if using SGD optimizer\n",
        "    # if epoch+1 == 50:\n",
        "    #     lr_rate = lr_rate/10\n",
        "    #     encoder_optimizer1 = torch.optim.SGD(encoder.parameters(), lr=lr_rate,momentum=0.9)\n",
        "    #     decoder_optimizer1 = torch.optim.SGD(attn_decoder1.parameters(), lr=lr_rate,momentum=0.9)\n",
        "    # if epoch+1 == 75:\n",
        "    #     lr_rate = lr_rate/10\n",
        "    #     encoder_optimizer1 = torch.optim.SGD(encoder.parameters(), lr=lr_rate,momentum=0.9)\n",
        "    #     decoder_optimizer1 = torch.optim.SGD(attn_decoder1.parameters(), lr=lr_rate,momentum=0.9)\n",
        "\n",
        "\n",
        "    running_loss=0\n",
        "    whole_loss = 0\n",
        "\n",
        "    encoder.train(mode=True)\n",
        "    attn_decoder1.train(mode=True)\n",
        "\n",
        "    # this is the train\n",
        "#     for step,(x,y) in tqdm(enumerate(train_loader)):\n",
        "    for step,(x,y) in enumerate(train_loader):\n",
        "        if x.size()[0]<batch_size:\n",
        "            break\n",
        "        h_mask = []\n",
        "        w_mask = []\n",
        "        for i in x:\n",
        "            #h*w\n",
        "            size_mask = i[1].size()\n",
        "            s_w = str(i[1][0])\n",
        "            s_h = str(i[1][:,1])\n",
        "            w = s_w.count('1')\n",
        "            h = s_h.count('1')\n",
        "            h_comp = int(h/16)+1\n",
        "            w_comp = int(w/16)+1\n",
        "            h_mask.append(h_comp)\n",
        "            w_mask.append(w_comp)\n",
        "\n",
        "        x = x.cuda()\n",
        "        y = y.cuda()\n",
        "        # out is CNN featuremaps\n",
        "        output_highfeature = encoder(x)\n",
        "        x_mean=[]\n",
        "        for i in output_highfeature:\n",
        "            x_mean.append(float(torch.mean(i)))\n",
        "        # x_mean = torch.mean(output_highfeature)\n",
        "        # x_mean = float(x_mean)\n",
        "        for i in range(batch_size):\n",
        "            decoder_hidden_init[i] = decoder_hidden_init[i]*x_mean[i]\n",
        "            decoder_hidden_init[i] = torch.tanh(decoder_hidden_init[i])\n",
        "\n",
        "        # dense_input is height and output_area is width which is bb\n",
        "        output_area1 = output_highfeature.size()\n",
        "\n",
        "        output_area = output_area1[3]\n",
        "        dense_input = output_area1[2]\n",
        "        target_length = y.size()[1]\n",
        "        attention_sum_init = torch.zeros(batch_size,1,dense_input,output_area).cuda()\n",
        "        decoder_attention_init = torch.zeros(batch_size,1,dense_input,output_area).cuda()\n",
        "\n",
        "        running_loss += my_train(target_length,attn_decoder1,output_highfeature,\n",
        "                                output_area,y,criterion,encoder_optimizer1,decoder_optimizer1,x_mean,dense_input,h_mask,w_mask,gpu,\n",
        "                                decoder_input_init,decoder_hidden_init,attention_sum_init,decoder_attention_init)\n",
        "\n",
        "        \n",
        "        if step % 20 == 19:\n",
        "            pre = ((step+1)/len_train)*100*batch_size\n",
        "            whole_loss += running_loss\n",
        "            running_loss = running_loss/(batch_size*20)\n",
        "            print('epoch is %d, lr rate is %.5f, te is %.3f, batch_size is %d, loading for %.3f%%, running_loss is %f' %(epoch,lr_rate,teacher_forcing_ratio, batch_size,pre,running_loss))\n",
        "            # with open(\"training_data/running_loss_%.5f_pre_GN_te05_d02_all.txt\" %(lr_rate),\"a\") as f:\n",
        "            #     f.write(\"%s\\n\"%(str(running_loss)))\n",
        "            running_loss = 0\n",
        "\n",
        "    loss_all_out = whole_loss / len_train\n",
        "    print(\"epoch is %d, the whole loss is %f\" % (epoch, loss_all_out))\n",
        "    # with open(\"training_data/whole_loss_%.5f_pre_GN_te05_d02_all.txt\" % (lr_rate), \"a\") as f:\n",
        "    #     f.write(\"%s\\n\" % (str(loss_all_out)))\n",
        "\n",
        "    # this is the prediction and compute wer loss\n",
        "    total_dist = 0\n",
        "    total_label = 0\n",
        "    total_line = 0\n",
        "    total_line_rec = 0\n",
        "    whole_loss_t = 0\n",
        "\n",
        "    encoder.eval()\n",
        "    attn_decoder1.eval()\n",
        "    print('Now, begin testing!!')\n",
        "\n",
        "#     for step_t, (x_t, y_t) in tqdm(enumerate(test_loader)):\n",
        "    for step_t, (x_t, y_t) in enumerate(test_loader):\n",
        "        x_real_high = x_t.size()[2]\n",
        "        x_real_width = x_t.size()[3]\n",
        "        if x_t.size()[0]<batch_size_t:\n",
        "            break\n",
        "        print('testing for %.3f%%'%(step_t*100*batch_size_t/len_test),end='\\r')\n",
        "        h_mask_t = []\n",
        "        w_mask_t = []\n",
        "        for i in x_t:\n",
        "            #h*w\n",
        "            size_mask_t = i[1].size()\n",
        "            s_w_t = str(i[1][0])\n",
        "            s_h_t = str(i[1][:,1])\n",
        "            w_t = s_w_t.count('1')\n",
        "            h_t = s_h_t.count('1')\n",
        "            h_comp_t = int(h_t/16)+1\n",
        "            w_comp_t = int(w_t/16)+1\n",
        "            h_mask_t.append(h_comp_t)\n",
        "            w_mask_t.append(w_comp_t)\n",
        "\n",
        "        x_t = x_t.cuda()\n",
        "        y_t = y_t.cuda()\n",
        "        output_highfeature_t = encoder(x_t)\n",
        "\n",
        "        x_mean_t = torch.mean(output_highfeature_t)\n",
        "        x_mean_t = float(x_mean_t)\n",
        "        output_area_t1 = output_highfeature_t.size()\n",
        "        output_area_t = output_area_t1[3]\n",
        "        dense_input = output_area_t1[2]\n",
        "\n",
        "        decoder_input_t = torch.LongTensor([111]*batch_size_t)\n",
        "        decoder_input_t = decoder_input_t.cuda()\n",
        "        decoder_hidden_t = torch.randn(batch_size_t, 1, hidden_size).cuda()\n",
        "        nn.init.xavier_uniform_(decoder_hidden_t)\n",
        "\n",
        "        x_mean_t=[]\n",
        "        for i in output_highfeature_t:\n",
        "            x_mean_t.append(float(torch.mean(i)))\n",
        "        # x_mean = torch.mean(output_highfeature)\n",
        "        # x_mean = float(x_mean)\n",
        "        for i in range(batch_size_t):\n",
        "            decoder_hidden_t[i] = decoder_hidden_t[i]*x_mean_t[i]\n",
        "            decoder_hidden_t[i] = torch.tanh(decoder_hidden_t[i])\n",
        "\n",
        "        prediction = torch.zeros(batch_size_t,maxlen)\n",
        "        #label = torch.zeros(batch_size_t,maxlen)\n",
        "        prediction_sub = []\n",
        "        label_sub = []\n",
        "        decoder_attention_t = torch.zeros(batch_size_t,1,dense_input,output_area_t).cuda()\n",
        "        attention_sum_t = torch.zeros(batch_size_t,1,dense_input,output_area_t).cuda()\n",
        "        flag_z_t = [0]*batch_size_t\n",
        "        loss_t = 0\n",
        "        m = torch.nn.ZeroPad2d((0,maxlen-y_t.size()[1],0,0))\n",
        "        y_t = m(y_t)\n",
        "        for i in range(maxlen):\n",
        "            decoder_output, decoder_hidden_t, decoder_attention_t, attention_sum_t = attn_decoder1(decoder_input_t,\n",
        "                                                                                             decoder_hidden_t,\n",
        "                                                                                             output_highfeature_t,\n",
        "                                                                                             output_area_t,\n",
        "                                                                                             attention_sum_t,\n",
        "                                                                                             decoder_attention_t,dense_input,batch_size_t,h_mask_t,w_mask_t,gpu)\n",
        "\n",
        "            ### you can see the attention when testing\n",
        "\n",
        "            # print('this is',i)\n",
        "            # for i in range(batch_size_t):\n",
        "            #     x_real = numpy.array(x_t[i][0].data.cpu())\n",
        "\n",
        "            #     show = numpy.array(decoder_attention_t[i][0].data.cpu())\n",
        "            #     show = imresize(show,(x_real_width,x_real_high))\n",
        "            #     k_max = show.max()\n",
        "            #     show = show/k_max\n",
        "\n",
        "            #     show_x = x_real+show\n",
        "            #     plt.imshow(show_x, interpolation='nearest', cmap='gray_r')\n",
        "            #     plt.show()\n",
        "            \n",
        "            topv,topi = torch.max(decoder_output,2)\n",
        "            # if torch.sum(y_t[0,:,i])==0:\n",
        "            #     y_t = y_t.squeeze(0)\n",
        "            #     break\n",
        "            if torch.sum(topi)==0:\n",
        "                break\n",
        "            decoder_input_t = topi\n",
        "            decoder_input_t = decoder_input_t.view(batch_size_t)\n",
        "\n",
        "            # prediction\n",
        "            prediction[:,i] = decoder_input_t\n",
        "\n",
        "        for i in range(batch_size_t):\n",
        "            for j in range(maxlen):\n",
        "                if int(prediction[i][j]) ==0:\n",
        "                    break\n",
        "                else:\n",
        "                    prediction_sub.append(int(prediction[i][j]))\n",
        "            if len(prediction_sub)<maxlen:\n",
        "                prediction_sub.append(0)\n",
        "\n",
        "            for k in range(y_t.size()[1]):\n",
        "                if int(y_t[i][k]) ==0:\n",
        "                    break\n",
        "                else:\n",
        "                    label_sub.append(int(y_t[i][k]))\n",
        "            label_sub.append(0)\n",
        "\n",
        "            dist, llen = cmp_result(label_sub, prediction_sub)\n",
        "            total_dist += dist\n",
        "            total_label += llen\n",
        "            total_line += 1\n",
        "            if dist == 0:\n",
        "                total_line_rec = total_line_rec+ 1\n",
        "\n",
        "            label_sub = []\n",
        "            prediction_sub = []\n",
        "\n",
        "    print('total_line_rec is',total_line_rec)\n",
        "    wer = float(total_dist) / total_label\n",
        "    sacc = float(total_line_rec) / total_line\n",
        "    print('wer is %.5f' % (wer))\n",
        "    print('sacc is %.5f ' % (sacc))\n",
        "    # print('whole loss is %.5f'%(whole_loss_t/925))\n",
        "    # with open(\"training_data/wer_%.5f_pre_GN_te05_d02_all.txt\" % (lr_rate), \"a\") as f:\n",
        "    #     f.write(\"%s\\n\" % (str(wer)))\n",
        "\n",
        "    if (sacc > exprate):\n",
        "        exprate = sacc\n",
        "        print(exprate)\n",
        "        print(\"saving the model....\")\n",
        "        print('encoder_lr%.5f_GN_te1_d05_SGD_bs6_mask_conv_bn_b_xavier.pkl' %(lr_rate))\n",
        "        torch.save(encoder.state_dict(), f'{PATH}model/encoder_v1.pkl'%(lr_rate))\n",
        "        torch.save(attn_decoder1.state_dict(), f'{PATH}model/attn_decoder_v1.pkl'%(lr_rate))\n",
        "        print(\"done\")\n",
        "        flag = 0\n",
        "    else:\n",
        "        flag = flag+1\n",
        "        print('the best is %f' % (exprate))\n",
        "        print('the loss is bigger than before,so do not save the model')\n",
        "\n",
        "    if flag == 10:\n",
        "        lr_rate = lr_rate*0.1\n",
        "        flag = 0"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/5 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 1.436%, running_loss is 5.015397\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 2.871%, running_loss is 4.149282\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 4.307%, running_loss is 5.349053\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 5.742%, running_loss is 4.430771\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 7.178%, running_loss is 6.311954\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 8.613%, running_loss is 6.287633\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 10.049%, running_loss is 4.872543\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 11.485%, running_loss is 6.571854\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 12.920%, running_loss is 4.774577\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 14.356%, running_loss is 6.366533\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 15.791%, running_loss is 5.497327\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 17.227%, running_loss is 5.928962\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 18.663%, running_loss is 3.955948\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 20.098%, running_loss is 4.127971\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 21.534%, running_loss is 4.099347\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 22.969%, running_loss is 4.903491\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 24.405%, running_loss is 3.755364\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 25.840%, running_loss is 6.386693\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 27.276%, running_loss is 6.026866\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 28.712%, running_loss is 5.446581\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 30.147%, running_loss is 5.477351\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 31.583%, running_loss is 6.816938\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 33.018%, running_loss is 5.954819\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 34.454%, running_loss is 5.850532\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 35.889%, running_loss is 6.425564\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 37.325%, running_loss is 5.215057\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 38.761%, running_loss is 4.358123\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 40.196%, running_loss is 6.415337\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 41.632%, running_loss is 4.889730\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 43.067%, running_loss is 4.637101\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 44.503%, running_loss is 5.557228\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 45.939%, running_loss is 4.866389\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 47.374%, running_loss is 8.627742\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 48.810%, running_loss is 4.714643\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 50.245%, running_loss is 7.904326\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 51.681%, running_loss is 5.914719\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 53.116%, running_loss is 6.338880\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 54.552%, running_loss is 5.193889\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 55.988%, running_loss is 5.599801\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 57.423%, running_loss is 4.473834\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 58.859%, running_loss is 5.230767\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 60.294%, running_loss is 5.433557\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 61.730%, running_loss is 5.707469\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 63.165%, running_loss is 10.010314\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 64.601%, running_loss is 5.678075\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 66.037%, running_loss is 6.010463\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 67.472%, running_loss is 7.069790\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 68.908%, running_loss is 5.191345\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 70.343%, running_loss is 5.552844\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 71.779%, running_loss is 4.961839\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 73.214%, running_loss is 5.784384\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 74.650%, running_loss is 5.738039\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 76.086%, running_loss is 4.817941\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 77.521%, running_loss is 5.641852\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 78.957%, running_loss is 5.657722\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 80.392%, running_loss is 6.872669\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 81.828%, running_loss is 5.339089\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 83.264%, running_loss is 5.871894\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 84.699%, running_loss is 7.300274\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 86.135%, running_loss is 8.142171\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 87.570%, running_loss is 4.461420\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 89.006%, running_loss is 5.241874\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 90.441%, running_loss is 6.840804\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 91.877%, running_loss is 5.909783\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 93.313%, running_loss is 4.210871\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 94.748%, running_loss is 3.594066\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 96.184%, running_loss is 5.233015\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 97.619%, running_loss is 5.205490\n",
            "epoch is 0, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 99.055%, running_loss is 4.943088\n",
            "epoch is 0, the whole loss is 5.557742\n",
            "Now, begin testing!!\n",
            "total_line_rec is 108\n",
            "wer is 0.42426\n",
            "sacc is 0.11688 \n",
            "0.11688311688311688\n",
            "saving the model....\n",
            "encoder_lr0.00100_GN_te1_d05_SGD_bs6_mask_conv_bn_b_xavier.pkl\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 1/5 [08:44<34:56, 524.12s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "done\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 1.436%, running_loss is 4.184375\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 2.871%, running_loss is 4.677055\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 4.307%, running_loss is 5.193573\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 5.742%, running_loss is 3.816893\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 7.178%, running_loss is 3.945412\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 8.613%, running_loss is 6.096613\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 10.049%, running_loss is 4.498590\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 11.485%, running_loss is 3.982410\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 12.920%, running_loss is 5.641692\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 14.356%, running_loss is 5.632161\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 15.791%, running_loss is 5.176009\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 17.227%, running_loss is 4.375731\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 18.663%, running_loss is 5.731225\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 20.098%, running_loss is 3.218800\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 21.534%, running_loss is 5.534961\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 22.969%, running_loss is 4.438735\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 24.405%, running_loss is 5.390726\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 25.840%, running_loss is 5.651150\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 27.276%, running_loss is 6.399215\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 28.712%, running_loss is 3.551911\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 30.147%, running_loss is 3.756784\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 31.583%, running_loss is 4.329802\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 33.018%, running_loss is 3.573141\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 34.454%, running_loss is 6.443806\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 35.889%, running_loss is 4.307548\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 37.325%, running_loss is 4.598077\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 38.761%, running_loss is 3.745735\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 40.196%, running_loss is 5.179065\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 41.632%, running_loss is 4.725707\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 43.067%, running_loss is 4.404011\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 44.503%, running_loss is 3.481786\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 45.939%, running_loss is 5.232972\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 47.374%, running_loss is 8.645230\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 48.810%, running_loss is 4.984467\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 50.245%, running_loss is 4.099311\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 51.681%, running_loss is 4.395951\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 53.116%, running_loss is 4.832292\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 54.552%, running_loss is 5.241298\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 55.988%, running_loss is 4.425194\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 57.423%, running_loss is 4.781465\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 58.859%, running_loss is 3.907037\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 60.294%, running_loss is 4.437792\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 61.730%, running_loss is 4.584712\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 63.165%, running_loss is 6.678295\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 64.601%, running_loss is 5.030411\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 66.037%, running_loss is 4.288965\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 67.472%, running_loss is 3.574293\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 68.908%, running_loss is 5.553482\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 70.343%, running_loss is 5.705835\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 71.779%, running_loss is 4.948495\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 73.214%, running_loss is 4.282816\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 74.650%, running_loss is 5.032300\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 76.086%, running_loss is 4.401070\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 77.521%, running_loss is 4.840052\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 78.957%, running_loss is 3.738810\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 80.392%, running_loss is 4.618656\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 81.828%, running_loss is 5.114881\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 83.264%, running_loss is 5.017689\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 84.699%, running_loss is 5.765106\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 86.135%, running_loss is 5.155504\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 87.570%, running_loss is 4.106512\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 89.006%, running_loss is 5.265973\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 90.441%, running_loss is 4.598795\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 91.877%, running_loss is 5.477136\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 93.313%, running_loss is 5.652304\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 94.748%, running_loss is 4.421125\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 96.184%, running_loss is 5.853325\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 97.619%, running_loss is 6.669779\n",
            "epoch is 1, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 99.055%, running_loss is 4.735660\n",
            "epoch is 1, the whole loss is 4.820352\n",
            "Now, begin testing!!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 2/5 [17:30<26:14, 524.88s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "total_line_rec is 105\n",
            "wer is 0.44829\n",
            "sacc is 0.11364 \n",
            "the best is 0.116883\n",
            "the loss is bigger than before,so do not save the model\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 1.436%, running_loss is 3.616501\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 2.871%, running_loss is 3.688004\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 4.307%, running_loss is 4.100421\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 5.742%, running_loss is 4.031174\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 7.178%, running_loss is 3.511727\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 8.613%, running_loss is 9.340674\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 10.049%, running_loss is 3.746660\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 11.485%, running_loss is 3.686058\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 12.920%, running_loss is 4.345363\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 14.356%, running_loss is 3.414509\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 15.791%, running_loss is 3.004767\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 17.227%, running_loss is 4.646687\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 18.663%, running_loss is 3.077833\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 20.098%, running_loss is 6.507670\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 21.534%, running_loss is 3.805063\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 22.969%, running_loss is 4.540640\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 24.405%, running_loss is 4.830330\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 25.840%, running_loss is 4.306508\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 27.276%, running_loss is 4.175361\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 28.712%, running_loss is 4.981057\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 30.147%, running_loss is 4.734833\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 31.583%, running_loss is 4.711344\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 33.018%, running_loss is 2.848088\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 34.454%, running_loss is 3.721862\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 35.889%, running_loss is 3.259593\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 37.325%, running_loss is 5.043091\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 38.761%, running_loss is 5.488053\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 40.196%, running_loss is 3.238016\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 41.632%, running_loss is 6.704456\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 43.067%, running_loss is 6.283818\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 44.503%, running_loss is 3.145887\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 45.939%, running_loss is 3.957683\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 47.374%, running_loss is 4.123865\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 48.810%, running_loss is 3.307756\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 50.245%, running_loss is 5.705123\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 51.681%, running_loss is 3.270100\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 53.116%, running_loss is 3.460281\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 54.552%, running_loss is 3.707384\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 55.988%, running_loss is 4.145469\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 57.423%, running_loss is 2.793226\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 58.859%, running_loss is 4.059872\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 60.294%, running_loss is 5.068888\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 61.730%, running_loss is 6.394129\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 63.165%, running_loss is 4.567701\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 64.601%, running_loss is 5.893299\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 66.037%, running_loss is 3.517205\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 67.472%, running_loss is 3.436022\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 68.908%, running_loss is 5.096525\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 70.343%, running_loss is 3.451966\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 71.779%, running_loss is 3.727399\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 73.214%, running_loss is 5.031286\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 74.650%, running_loss is 3.082576\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 76.086%, running_loss is 4.246428\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 77.521%, running_loss is 3.662252\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 78.957%, running_loss is 5.317936\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 80.392%, running_loss is 7.080791\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 81.828%, running_loss is 3.572744\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 83.264%, running_loss is 4.114944\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 84.699%, running_loss is 4.872501\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 86.135%, running_loss is 4.528534\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 87.570%, running_loss is 5.678458\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 89.006%, running_loss is 5.627028\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 90.441%, running_loss is 4.067881\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 91.877%, running_loss is 5.474328\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 93.313%, running_loss is 4.114108\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 94.748%, running_loss is 4.770332\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 96.184%, running_loss is 3.951253\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 97.619%, running_loss is 5.476836\n",
            "epoch is 2, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 99.055%, running_loss is 3.852646\n",
            "epoch is 2, the whole loss is 4.374822\n",
            "Now, begin testing!!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 3/5 [26:15<17:29, 524.79s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "total_line_rec is 110\n",
            "wer is 0.43931\n",
            "sacc is 0.11905 \n",
            "0.11904761904761904\n",
            "saving the model....\n",
            "encoder_lr0.00100_GN_te1_d05_SGD_bs6_mask_conv_bn_b_xavier.pkl\n",
            "done\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 1.436%, running_loss is 2.371705\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 2.871%, running_loss is 4.839333\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 4.307%, running_loss is 4.573723\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 5.742%, running_loss is 4.756245\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 7.178%, running_loss is 4.491072\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 8.613%, running_loss is 5.850393\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 10.049%, running_loss is 3.739105\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 11.485%, running_loss is 2.550779\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 12.920%, running_loss is 5.143190\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 14.356%, running_loss is 3.811973\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 15.791%, running_loss is 3.945697\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 17.227%, running_loss is 3.279554\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 18.663%, running_loss is 3.687686\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 20.098%, running_loss is 3.897060\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 21.534%, running_loss is 3.527321\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 22.969%, running_loss is 5.802855\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 24.405%, running_loss is 4.167408\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 25.840%, running_loss is 5.170401\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 27.276%, running_loss is 5.258572\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 28.712%, running_loss is 5.452171\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 30.147%, running_loss is 3.360592\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 31.583%, running_loss is 5.896872\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 33.018%, running_loss is 2.828970\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 34.454%, running_loss is 3.303427\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 35.889%, running_loss is 2.743462\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 37.325%, running_loss is 3.548703\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 38.761%, running_loss is 2.903091\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 40.196%, running_loss is 4.541821\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 41.632%, running_loss is 4.099147\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 43.067%, running_loss is 5.162129\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 44.503%, running_loss is 2.655750\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 45.939%, running_loss is 3.571146\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 47.374%, running_loss is 4.338759\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 48.810%, running_loss is 3.001722\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 50.245%, running_loss is 5.875647\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 51.681%, running_loss is 6.033715\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 53.116%, running_loss is 3.253237\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 54.552%, running_loss is 5.100882\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 55.988%, running_loss is 2.956743\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 57.423%, running_loss is 3.651178\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 58.859%, running_loss is 2.774822\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 60.294%, running_loss is 3.409414\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 61.730%, running_loss is 3.421942\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 63.165%, running_loss is 3.583319\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 64.601%, running_loss is 4.356510\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 66.037%, running_loss is 4.197551\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 67.472%, running_loss is 3.490236\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 68.908%, running_loss is 3.106882\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 70.343%, running_loss is 3.805376\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 71.779%, running_loss is 3.573886\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 73.214%, running_loss is 3.818554\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 74.650%, running_loss is 4.030920\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 76.086%, running_loss is 3.583012\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 77.521%, running_loss is 4.351202\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 78.957%, running_loss is 3.152124\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 80.392%, running_loss is 4.546689\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 81.828%, running_loss is 4.155479\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 83.264%, running_loss is 5.973963\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 84.699%, running_loss is 3.938597\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 86.135%, running_loss is 3.179901\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 87.570%, running_loss is 4.010601\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 89.006%, running_loss is 3.709588\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 90.441%, running_loss is 3.849522\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 91.877%, running_loss is 3.367245\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 93.313%, running_loss is 3.449704\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 94.748%, running_loss is 3.713830\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 96.184%, running_loss is 3.773890\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 97.619%, running_loss is 3.944902\n",
            "epoch is 3, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 99.055%, running_loss is 3.242390\n",
            "epoch is 3, the whole loss is 3.942892\n",
            "Now, begin testing!!\n",
            "total_line_rec is 138\n",
            "wer is 0.37891\n",
            "sacc is 0.14935 \n",
            "0.14935064935064934\n",
            "saving the model....\n",
            "encoder_lr0.00100_GN_te1_d05_SGD_bs6_mask_conv_bn_b_xavier.pkl\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 4/5 [35:00<08:44, 524.78s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "done\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 1.436%, running_loss is 5.019246\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 2.871%, running_loss is 2.654645\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 4.307%, running_loss is 3.109702\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 5.742%, running_loss is 2.696803\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 7.178%, running_loss is 2.163007\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 8.613%, running_loss is 4.904347\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 10.049%, running_loss is 2.494650\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 11.485%, running_loss is 2.425129\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 12.920%, running_loss is 2.827483\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 14.356%, running_loss is 3.207170\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 15.791%, running_loss is 3.244547\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 17.227%, running_loss is 3.679399\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 18.663%, running_loss is 4.301298\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 20.098%, running_loss is 3.879590\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 21.534%, running_loss is 4.708226\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 22.969%, running_loss is 5.273235\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 24.405%, running_loss is 2.280627\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 25.840%, running_loss is 3.518796\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 27.276%, running_loss is 3.217992\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 28.712%, running_loss is 3.459057\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 30.147%, running_loss is 4.470029\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 31.583%, running_loss is 3.411909\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 33.018%, running_loss is 3.296773\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 34.454%, running_loss is 3.127509\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 35.889%, running_loss is 4.387609\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 37.325%, running_loss is 3.034922\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 38.761%, running_loss is 3.257030\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 40.196%, running_loss is 3.200146\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 41.632%, running_loss is 5.574848\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 43.067%, running_loss is 4.152859\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 44.503%, running_loss is 3.126056\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 45.939%, running_loss is 2.170675\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 47.374%, running_loss is 3.023567\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 48.810%, running_loss is 2.877826\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 50.245%, running_loss is 5.570435\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 51.681%, running_loss is 6.644985\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 53.116%, running_loss is 3.883655\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 54.552%, running_loss is 2.560535\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 55.988%, running_loss is 4.052124\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 57.423%, running_loss is 4.353206\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 58.859%, running_loss is 2.851692\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 60.294%, running_loss is 5.491882\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 61.730%, running_loss is 5.499432\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 63.165%, running_loss is 5.616112\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 64.601%, running_loss is 3.497100\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 66.037%, running_loss is 3.382439\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 67.472%, running_loss is 4.978237\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 68.908%, running_loss is 3.016810\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 70.343%, running_loss is 2.997170\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 71.779%, running_loss is 4.216116\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 73.214%, running_loss is 5.314510\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 74.650%, running_loss is 6.567113\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 76.086%, running_loss is 5.178221\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 77.521%, running_loss is 3.328105\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 78.957%, running_loss is 2.871436\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 80.392%, running_loss is 3.801016\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 81.828%, running_loss is 3.235654\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 83.264%, running_loss is 2.522882\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 84.699%, running_loss is 3.356882\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 86.135%, running_loss is 3.356058\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 87.570%, running_loss is 3.794455\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 89.006%, running_loss is 4.790595\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 90.441%, running_loss is 3.847651\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 91.877%, running_loss is 2.475688\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 93.313%, running_loss is 6.022955\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 94.748%, running_loss is 5.005471\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 96.184%, running_loss is 4.846509\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 97.619%, running_loss is 3.160441\n",
            "epoch is 4, lr rate is 0.00100, te is 1.000, batch_size is 6, loading for 99.055%, running_loss is 3.675117\n",
            "epoch is 4, the whole loss is 3.789057\n",
            "Now, begin testing!!\n",
            "total_line_rec is 166\n",
            "wer is 0.33762\n",
            "sacc is 0.17965 \n",
            "0.17965367965367965\n",
            "saving the model....\n",
            "encoder_lr0.00100_GN_te1_d05_SGD_bs6_mask_conv_bn_b_xavier.pkl\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [43:45<00:00, 525.01s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIip6qBynGqX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 12,
      "outputs": []
    }
  ]
}